{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be18eea",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396db8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- helper used by the loader ----------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_intraday_data(df: pd.DataFrame, tz: str = \"America/New_York\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize raw Alpaca 1-min bars so later EDA/backtests see the same shape.\n",
    "\n",
    "    Input df must at least have: ['timestamp','close'] (plus open/high/low/volume/vwap if available).\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1) Parse timestamps and convert to NYSE timezone.\n",
    "    2) Keep only Regular Trading Hours (09:30–16:00).\n",
    "    3) Sort chronologically.\n",
    "    4) Add day label and compute intraday returns:\n",
    "       - 'return'     : simple pct change\n",
    "       - 'log_return' : log return\n",
    "\n",
    "    Note: We *don’t* compute overnight returns here. That’s done later\n",
    "    in `clean_and_enrich`, which also clips extreme intraday returns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) robust tz handling (works whether timestamp is naive, UTC, or already tz-aware)\n",
    "    ts = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    if ts.dt.tz is None:\n",
    "        ts = ts.dt.tz_localize(\"UTC\").dt.tz_convert(tz)\n",
    "    else:\n",
    "        ts = ts.dt.tz_convert(tz)\n",
    "    df[\"timestamp\"] = ts\n",
    "\n",
    "    # 2) RTH filter\n",
    "    df = df.set_index(\"timestamp\").between_time(\"09:30\", \"16:00\").reset_index()\n",
    "\n",
    "    # 3) sort\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # 4) labels + returns\n",
    "    df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "    df[\"return\"] = df[\"close\"].pct_change()\n",
    "    df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load multiple years/symbols\n",
    "# -----------------------\n",
    "symbols = [\"SPY\", \"QQQ\"]   # can extend this list\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "\n",
    "data_dir = Path(\"../data/raw\")\n",
    "\n",
    "all_dfs = {}\n",
    "\n",
    "for symbol in symbols:\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        file = data_dir / f\"{symbol}_1min_{year}.parquet\"\n",
    "        if file.exists():\n",
    "            df_year = pd.read_parquet(file).reset_index()\n",
    "            df_year = prepare_intraday_data(df_year)\n",
    "            dfs.append(df_year)\n",
    "    if dfs:\n",
    "        df_symbol = pd.concat(dfs, ignore_index=True)\n",
    "        all_dfs[symbol] = df_symbol\n",
    "        print(f\"Loaded {symbol}: {len(df_symbol)} rows across {len(dfs)} years\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helper: clean + add overnight returns\n",
    "# -----------------------\n",
    "def clean_and_enrich(df, clip=0.05):\n",
    "    \"\"\"\n",
    "    Cleans data by clipping extreme log returns and adds overnight returns.\n",
    "    Keeps intraday returns intact except for the first bar of each day.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Overnight returns ---\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['overnight_return'] = pd.NA\n",
    "    df['overnight_log_return'] = pd.NA\n",
    "    \n",
    "    for date, group in df.groupby('date'):\n",
    "        if len(group) > 0:\n",
    "            first_idx = group.index[0]\n",
    "            prev_idx = first_idx - 1\n",
    "            if prev_idx in df.index:\n",
    "                prev_close = df.loc[prev_idx, 'close']\n",
    "                this_close = df.loc[first_idx, 'close']\n",
    "                raw_r = (this_close / prev_close) - 1\n",
    "                log_r = np.log(this_close / prev_close)\n",
    "                df.loc[first_idx, 'overnight_return'] = raw_r\n",
    "                df.loc[first_idx, 'overnight_log_return'] = log_r\n",
    "\n",
    "    # Drop first intraday return each day (so overnight isn’t mixed in)\n",
    "    first_rows = df.groupby('date').head(1).index\n",
    "    df.loc[first_rows, 'return'] = pd.NA\n",
    "    df.loc[first_rows, 'log_return'] = pd.NA\n",
    "\n",
    "    # --- Clean extreme intraday returns ---\n",
    "    df.loc[df['log_return'].abs() > clip, 'log_return'] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2. Run analyses per symbol\n",
    "# -----------------------\n",
    "for symbol, df in all_dfs.items():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Analysis for {symbol}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Clean + enrich\n",
    "    df = clean_and_enrich(df, clip=0.05)\n",
    "\n",
    "    # Plot close vs VWAP\n",
    "    fig = px.line(\n",
    "        df,\n",
    "        x=\"timestamp\",\n",
    "        y=[\"close\", \"vwap\"],\n",
    "        title=f\"{symbol} Close vs VWAP ({years[0]}–{years[-1]}, 1-min)\"\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Intraday volume curve\n",
    "    df['time'] = df['timestamp'].dt.time\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    avg_volume = df.groupby('time')['volume'].mean().reset_index()\n",
    "    fig = px.line(\n",
    "        avg_volume,\n",
    "        x=\"time\", y=\"volume\",\n",
    "        title=f\"Average Intraday Volume Curve ({symbol})\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Return distribution + Normal overlay\n",
    "    mu, sigma = df['log_return'].mean(), df['log_return'].std()\n",
    "    fig = px.histogram(\n",
    "        df, x=\"log_return\", nbins=300,\n",
    "        title=f\"Distribution of 1-min log returns ({symbol})\",\n",
    "        histnorm=\"probability density\"\n",
    "    )\n",
    "    # Overlay normal distribution\n",
    "    x_vals = np.linspace(\n",
    "        df['log_return'].dropna().min(),\n",
    "        df['log_return'].dropna().max(),\n",
    "        500\n",
    "    )\n",
    "    normal_pdf = stats.norm.pdf(x_vals, mu, sigma)\n",
    "    fig.add_trace(go.Scatter(x=x_vals, y=normal_pdf, mode='lines',\n",
    "                             name='Normal PDF', line=dict(color=\"red\")))\n",
    "    fig.show()\n",
    "    \n",
    "    # Q-Q plot\n",
    "    sm.qqplot(df['log_return'].dropna(), line='s')\n",
    "    plt.title(f'Q-Q Plot of 1-min log returns ({symbol})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary stats\n",
    "    print(f\"Mean log return: {df['log_return'].mean()}\")\n",
    "    print(f\"Std log return: {df['log_return'].std()}\")\n",
    "    print(f\"Min log return: {df['log_return'].min()}\")\n",
    "    print(f\"Max log return: {df['log_return'].max()}\")\n",
    "    print(f\"Skew: {df['log_return'].skew()}\")\n",
    "    print(f\"Kurtosis: {df['log_return'].kurtosis()}\")\n",
    "    \n",
    "    # Intraday volatility curve\n",
    "    vol_curve = df.groupby('time')['log_return'].std()\n",
    "    fig = px.line(\n",
    "        x=vol_curve.index.astype(str),\n",
    "        y=vol_curve.values,\n",
    "        title=f\"Average Intraday Volatility Curve ({symbol})\",\n",
    "        labels={'x':'Time of day', 'y':'Std of 1-min log returns'}\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # ACF plots\n",
    "    returns = df['log_return'].dropna()\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "    sm.graphics.tsa.plot_acf(returns, lags=60, zero=False,\n",
    "                             missing='drop', auto_ylims=True, ax=ax[0])\n",
    "    ax[0].set_title(f\"ACF of 1-min log returns ({symbol})\")\n",
    "    sm.graphics.tsa.plot_acf(returns**2, lags=60, zero=False,\n",
    "                             missing='drop', auto_ylims=True, ax=ax[1])\n",
    "    ax[1].set_title(f\"ACF of Squared 1-min log returns ({symbol})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Cross-symbol relationships\n",
    "# -------------------------------\n",
    "if len(all_dfs) > 1:\n",
    "    # Align on timestamps\n",
    "    aligned = pd.DataFrame(index=pd.concat([df.set_index('timestamp') for df in all_dfs.values()]).index.unique())\n",
    "    for symbol, df in all_dfs.items():\n",
    "        aligned[symbol] = df.set_index('timestamp')['log_return']\n",
    "    \n",
    "    aligned = aligned.sort_index().dropna()\n",
    "    print(\"Cross-correlation matrix of log returns:\")\n",
    "    print(aligned.corr())\n",
    "    \n",
    "    fig = px.imshow(aligned.corr(),\n",
    "                    text_auto=\".2f\",\n",
    "                    title=\"Cross-correlation of log returns across symbols\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Optional: cross-correlogram between two symbols\n",
    "    s1, s2 = symbols[:2]\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    sm.graphics.tsa.plot_ccf(aligned[s1], aligned[s2], lags=60, auto_ylims=True, ax=ax)\n",
    "    ax.set_title(f\"Cross-correlation function: {s1} vs {s2}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# 4. Save cleaned and enriched data\n",
    "# ---------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for symbol, df in all_dfs.items():\n",
    "    # Clean one more time for safety\n",
    "    df_clean = clean_and_enrich(df, clip=0.05)\n",
    "    \n",
    "    # Save to processed directory\n",
    "    save_path = output_dir / f\"{symbol}_1min_clean.parquet\"\n",
    "    df_clean.to_parquet(save_path, index=False)\n",
    "    print(f\"✅ Saved cleaned data for {symbol}: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
